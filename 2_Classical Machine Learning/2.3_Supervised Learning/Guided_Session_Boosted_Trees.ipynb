{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš€ Welcome to the Boosting and Stacking Practice Session!  ðŸ“Šâœ¨\n",
        "\n",
        "\n",
        "\n",
        "In this practice, weâ€™ll dive into XGBoost, AdaBoost, and Stacking â€” three powerful techniques in ensemble learning used to boost model performance and robustness. You'll gain hands-on experience with each method by applying them to a real-world dataset and analyzing their impact on classification accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **Introduction:**\n",
        "- **Time Limit:** You have 60 minutes to complete the practice.\n",
        "- **Objective:** Implement XGBoost, AdaBoost, and a simple stacking ensemble to classify data. Youâ€™ll preprocess the dataset, train each model, fine-tune hyperparameters, and evaluate their performances. Document your approach, hyperparameters, and any insights you gather along the way.\n",
        "- **Presentation:** After completing the exercise, present your findings. Discuss each modelâ€™s accuracy, any differences in performance, and the benefits and challenges of each approach.\n",
        "- **Ask for Help:** If you need assistance at any point, feel free to ask the instructor for guidance or clarification.\n",
        "\n",
        "## **Why This Matters:**\n",
        "\n",
        "Boosting algorithms like XGBoost and AdaBoost are widely used in machine learning due to their ability to create strong models from weak learners. Stacking adds another layer of prediction by combining multiple models, helping to leverage the strengths of each. Mastering these techniques will equip you with essential tools for improving accuracy in complex tasks and enhance your understanding of ensemble methods.\n",
        "\n",
        "\n",
        "\n",
        "## **Recap** ðŸ“‘\n",
        "\n",
        "### XGBoost:\n",
        "XGBoost, short for eXtreme Gradient Boosting, is a powerful implementation of gradient boosting machines. It's known for its speed, performance, and versatility. Key features of XGBoost include:\n",
        "\n",
        "- **Optimized Performance**: XGBoost is optimized for both speed and performance, making it one of the most efficient implementations of gradient boosting.\n",
        "  \n",
        "- **Regularization**: It incorporates regularization techniques to prevent overfitting and improve generalization.\n",
        "  \n",
        "- **Flexibility**: XGBoost supports various objective functions and evaluation metrics, making it suitable for a wide range of tasks.\n",
        "\n",
        "### AdaBoost:\n",
        "\n",
        "AdaBoost, or Adaptive Boosting, combines multiple weak learners to create a strong overall model. Itâ€™s called \"adaptive\" because it adjusts the weights of each instance, assigning higher weights to those that were misclassified in previous rounds. While sensitive to noise, AdaBoost is effective for binary classification and can handle some multiclass problems, especially when the data is fairly clean.\n",
        "\n",
        "More details about the implementation check the following link:\n",
        "\n",
        " https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
        "\n",
        "### Stacking:\n",
        "Stacking, also known as stacked generalization, is an ensemble learning technique that combines the predictions of multiple base models to improve overall performance. Key aspects of stacking include:\n",
        "\n",
        "- **Model Combination**: Stacking combines predictions from diverse base models using a meta-learner to generate a final prediction.\n",
        "  \n",
        "- **Diversity**: Stacking leverages the diversity of base models to capture different aspects of the data and improve overall robustness.\n",
        "  \n",
        "- **Performance Boost**: By leveraging the strengths of multiple models, stacking often leads to performance improvements over individual models.\n",
        "\n",
        "In this notebook, we'll delve deeper into these concepts, exploring advanced techniques and practical implementations of XGBoost and stacking. Let's dive in and uncover the potential of these powerful ensemble learning methods!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Get ready to boost your modelâ€™s power and dive into the world of ensemble learning!\n",
        "\n",
        "Happy coding! ðŸ’»ðŸš€\n"
      ],
      "metadata": {
        "id": "K8dtQI2jXkAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Overview\n",
        "\n",
        "The dataset we'll be working with contains information about customers of a telecommunications company and whether they churned or not. Here's a brief overview of the dataset:\n",
        "\n",
        "- **CustomerID**: Unique identifier for each customer.\n",
        "- **Gender**: Gender of the customer (e.g., Male, Female).\n",
        "- **SeniorCitizen**: Indicates if the customer is a senior citizen (1) or not (0).\n",
        "- **Partner**: Whether the customer has a partner or not (Yes, No).\n",
        "- **Dependents**: Whether the customer has dependents or not (Yes, No).\n",
        "- **Tenure**: Number of months the customer has been with the company.\n",
        "- **PhoneService**: Whether the customer has a phone service or not (Yes, No).\n",
        "- **MultipleLines**: Whether the customer has multiple lines or not (Yes, No, No phone service).\n",
        "- **InternetService**: Type of internet service subscribed by the customer (DSL, Fiber optic, No).\n",
        "- **OnlineSecurity**: Whether the customer has online security or not (Yes, No, No internet service).\n",
        "- **OnlineBackup**: Whether the customer has online backup or not (Yes, No, No internet service).\n",
        "- **DeviceProtection**: Whether the customer has device protection or not (Yes, No, No internet service).\n",
        "- **TechSupport**: Whether the customer has tech support or not (Yes, No, No internet service).\n",
        "- **StreamingTV**: Whether the customer has streaming TV or not (Yes, No, No internet service).\n",
        "- **StreamingMovies**: Whether the customer has streaming movies or not (Yes, No, No internet service).\n",
        "- **Contract**: Type of contract the customer has (Month-to-month, One year, Two year).\n",
        "- **PaperlessBilling**: Whether the customer has paperless billing or not (Yes, No).\n",
        "- **PaymentMethod**: Payment method used by the customer (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)).\n",
        "- **MonthlyCharges**: Monthly charges incurred by the customer.\n",
        "- **TotalCharges**: Total charges incurred by the customer.\n",
        "- **Churn**: Whether the customer churned or not (Yes, No).\n",
        "\n",
        "### Data Exploration\n",
        "\n",
        "Before we proceed with modeling, let's explore the dataset to understand its structure and gain insights into the distribution of key variables. We'll check for any missing values and visualize the distribution of the target variable (Churn) to understand the imbalance in the dataset.\n"
      ],
      "metadata": {
        "id": "LHAJsayQzvVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First step we will load the necessairy libraries to use for this practice :\n",
        "\n"
      ],
      "metadata": {
        "id": "xhDqjaAE3PwV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z2mXzdPcXTpB"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import necessary libraries\n",
        "# Import pandas for data manipulation\n",
        "# Import train_test_split from sklearn.model_selection for splitting the dataset\n",
        "# Import xgboost and other necessary modules for modeling\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "hKW0Jies3vxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use pandas to load the dataset\n",
        "df = pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "szGTwwt_36vF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "c9e8b40d-7c7b-4de2-998c-0679b42c6769"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/WA_Fn-UseC_-Telco-Customer-Churn.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0d224b667325>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# use pandas to load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/WA_Fn-UseC_-Telco-Customer-Churn.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/WA_Fn-UseC_-Telco-Customer-Churn.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Display some rows of your dataset"
      ],
      "metadata": {
        "id": "b5Jhfg_a3-IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#display some rows to see the name of each column of your dataset\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "xo5MkD1W4GLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Data exploration"
      ],
      "metadata": {
        "id": "Nuwoi10ed6Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pandas functions to explore the dataset (e.g., df.info(), df.describe())\n",
        "\n",
        "df.info()\n",
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "diziqRd0d59-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values and handle them if necessary\n",
        "\n",
        "# Count missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Display columns with missing values\n",
        "print(missing_values[missing_values > 0])"
      ],
      "metadata": {
        "id": "jv2CEz4fd4fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of target variable 'Churn' (e.g., using seaborn's countplot)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Create a countplot to show the distribution of 'Churn'\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=df['Churn'], palette=\"coolwarm\")\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Distribution of Churn\", fontsize=14)\n",
        "plt.xlabel(\"Churn\", fontsize=12)\n",
        "plt.ylabel(\"Count\", fontsize=12)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f0ncPp4heEF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Data preprocessing"
      ],
      "metadata": {
        "id": "l5mD71DneOTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform necessary data preprocessing steps such as encoding categorical variables and scaling numerical features Hint: the previous practice (Decision Trees)\n",
        "# Loop through each column in the dataframe\n",
        "# Initialize the label encoder\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Initialize the Label Encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Loop through each column in the dataframe\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':  # Check if the column is categorical\n",
        "        df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop(columns=['Churn'])  # Features\n",
        "y = df['Churn']  # Target variable\n",
        "\n",
        "# Initialize the Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale numerical features\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# Display processed data\n",
        "print(X.head())\n"
      ],
      "metadata": {
        "id": "y_nrR6ZreNgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target variable (y)\n",
        "\n",
        "# Define features (X) by dropping the target column 'Churn'\n",
        "X = df.drop(columns=['Churn'])\n",
        "\n",
        "# Define target variable (y)\n",
        "y = df['Churn']"
      ],
      "metadata": {
        "id": "2uuKGrozeVet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets using train_test_split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Display the shape of the resulting sets\n",
        "print(f\"Training set: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n",
        "print(f\"Testing set: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")"
      ],
      "metadata": {
        "id": "jGjblqgFeXb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. **AdaBoost**\n",
        "\n",
        "- Create AdaBoost classifier with the following parameters : max_depth = 3, learning_rate = 0.1, n_estimators = 100"
      ],
      "metadata": {
        "id": "Sq0qFLn2ez5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an AdaBoost classifier (e.g., xgb.XGBClassifier)\n",
        "\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n"
      ],
      "metadata": {
        "id": "vThtvgCZe6qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the XGBoost model on the training data (e.g., using .fit() method)\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Display training completion message\n",
        "print(\"XGBoost model training completed!\")"
      ],
      "metadata": {
        "id": "62CW_VxYe-lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance on the testing data using appropriate metrics (e.g., accuracy_score, classification_report)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Display confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "lyFYdfxIfAbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Hyperparameter Tuning\n",
        "\n",
        "The code below may look complex at first glance, but it's aimed at helping us find the best combination of hyperparameters for our AdaBoost model.\n",
        "\n",
        "### What are Hyperparameters?\n",
        "\n",
        "Hyperparameters are settings that control the learning process of our machine learning model. They are not learned from the data, unlike model parameters, but rather set before the learning process begins.\n",
        "\n",
        "### Why Tuning Hyperparameters?\n",
        "\n",
        "Tuning hyperparameters is crucial for achieving optimal performance and preventing overfitting or underfitting of our model. By testing different combinations of hyperparameters, we can find the settings that result in the best performance on our dataset.\n",
        "\n",
        "### The Code Breakdown:\n",
        "\n",
        "- **max_depth, learning_rate, n_estimators:** These are all different hyperparameters that we're going to test. Each parameter controls a different aspect of the AdaBoost model's behavior.\n",
        "  \n",
        "- **Nested Loops:** The nested loops iterate through each combination of hyperparameter values. For example, the outer loop may iterate over max_depth values, while the inner loop may iterate over learning_rate values.\n",
        "\n",
        "- **Model Training and Evaluation:** Inside the loops, we initialize an AdaBoost model with the current hyperparameter values, train it on the training data, and evaluate its performance on the testing data using an accuracy score.\n",
        "\n",
        "- **Recording Results:** We keep track of the accuracy scores for each combination of hyperparameters. This allows us to compare the performance of different settings and identify the combination that yields the best results.\n",
        "\n",
        "By understanding and running this code, we can identify the hyperparameter settings that optimize our model's performance and achieve better predictive accuracy.\n"
      ],
      "metadata": {
        "id": "iljdtZDg8hfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Do:\n",
        "\n",
        "- Test the following values for max_depth : 3/5/7\n",
        "- Test the following values for learning rate : 0.1/0.01/0.001\n",
        "- Test the following values for number of estimators: 100/200/300\n",
        "- Make nested loop to test all combinations of these values."
      ],
      "metadata": {
        "id": "SlcopdMCinG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Hyperparameter tuning - AdaBoost\n",
        "# Test different values for the hyperparameters of AdaBoost and compare their performance scores.\n",
        "# Define a list of hyperparameters and their respective values to test\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "max_depth_values = [3, 5, 7]  # Example values for max_depth\n",
        "learning_rate_values = [0.01, 0.1, 1]  # Example values for learning_rate\n",
        "n_estimators_values = [50, 100, 150]  # Example values for n_estimators\n",
        "\n",
        "\n",
        "# Initialize lists to store the performance scores for each hyperparameter combination\n",
        "accuracy_scores = []\n",
        "\n",
        "# Loop through each hyperparameter combination\n",
        "for max_depth in max_depth_values:\n",
        "    for learning_rate in learning_rate_values:\n",
        "        for n_estimators in n_estimators_values:\n",
        "            # Initialize an AdaBoost classifier with the current hyperparameters\n",
        "            # Removed base_estimator argument as it is not supported\n",
        "            ada_classifier = AdaBoostClassifier(\n",
        "                                                learning_rate=learning_rate,\n",
        "                                                n_estimators=n_estimators)\n",
        "\n",
        "            # Train the AdaBoost model on the training data\n",
        "            ada_classifier.fit(X_train, y_train)\n",
        "\n",
        "            # Perform prediction\n",
        "            y_pred = ada_classifier.predict(X_test)\n",
        "\n",
        "            # Evaluate the model's performance on the testing data using accuracy score\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            # Append the accuracy score and corresponding hyperparameters to the list\n",
        "            accuracy_scores.append((max_depth, learning_rate, n_estimators, accuracy))\n",
        "\n",
        "# Sort the accuracy scores list in descending order based on accuracy score\n",
        "accuracy_scores.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "# Get the hyperparameters with the highest accuracy score\n",
        "best_hyperparameters = accuracy_scores[0]\n",
        "\n",
        "# Print the hyperparameters with the highest accuracy score\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)"
      ],
      "metadata": {
        "id": "zYBVae1BwT0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Model training with best hyperparameters - AdaBoost\n",
        "# Train the AdaBoost model on the training data using the best hyperparameters\n",
        "ada_best = AdaBoostClassifier(learning_rate=best_hyperparameters[1],\n",
        "                              n_estimators=best_hyperparameters[2])\n",
        "\n",
        "# Train model\n",
        "ada_best.fit(X_train, y_train)\n",
        "\n",
        "# Perform prediction\n",
        "y_pred = ada_best.predict(X_test)\n",
        "\n",
        "# Evaluate the AdaBoost model's performance on the testing data using accuracy score\n",
        "accuracy_ada_best = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy of the AdaBoost model with the best hyperparameters\n",
        "print(\"AdaBoost Model Accuracy with Best Hyperparameters:\", accuracy_ada_best)\n"
      ],
      "metadata": {
        "id": "mvo9qgytiDON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. XGBoost\n",
        "\n",
        "Implement XGBoost classifier with the best hyperparameters values of AdaBoost and make a comparaison between them.\n"
      ],
      "metadata": {
        "id": "7fJRUXzjhhbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the XGBoost classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Perform the test\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "\n",
        "# Print the accuracy of the XGBoost model\n",
        "print(f\"XGBoost Model Accuracy: {accuracy_xgb:.4f}\")"
      ],
      "metadata": {
        "id": "JPmLerUjhwm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Stacking"
      ],
      "metadata": {
        "id": "RCz-lnLsCRXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training - Stacking\n",
        "# Create a set of diverse base models (e.g., logistic regression, random forest, SVM)\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "base_models = [\n",
        "     ('logistic_regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('svm', SVC(kernel='linear', probability=True, random_state=42))\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "XGprd7Y6iVN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a stacking classifier with the base models and a meta-learner (e.g., another XGBoost model)\n",
        "stacked_model = StackingClassifier(estimators=base_models, final_estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n",
        "\n"
      ],
      "metadata": {
        "id": "hh39-5sfjIMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the stacked model on the base models' predictions and the corresponding true labels\n",
        "stacked_model.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "LfX81mrdjSZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model evaluation - Stacking\n",
        "# Use the trained stacked model to make predictions on the testing data\n",
        "stacked_model_predictions = stacked_model.predict(X_test)  # Fill in with the testing data (X_test)\n",
        "\n",
        "# Evaluate the stacked model's performance on the testing data using appropriate metrics\n",
        "stacked_model_accuracy = accuracy_score(y_test, stacked_model_predictions)  # Fill in with the true labels (y_test) and predictions (stacked_model_predictions)\n",
        "print(\"Stacked Model Accuracy:\", stacked_model_accuracy)\n"
      ],
      "metadata": {
        "id": "OTmKX5sDiTV0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}