{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Introduction to Machine Learning Homework\n",
        "\n",
        "Build a logistic regression model to classify the patients whether they got any type of food allergies using the Food Allergy Zenodo Dataset:\n",
        "\n",
        "https://claproduction.s3.eu-west-3.amazonaws.com/DataScience/3_classical_ml/food-allergy-analysis-Zenodo.csv"
      ],
      "metadata": {
        "id": "7iQ1qeggeO4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jnvX9sUMeMIZ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the dataset\n",
        "# Charger le dataset en DataFrame directement\n",
        "df = pd.read_csv('food-allergy-analysis-Zenodo.csv')\n",
        "\n",
        "# Afficher les premi√®res lignes du dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRS9TK8Of7Jo",
        "outputId": "5d38cff6-cb42-4cd9-8925-7bec2343b18f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   SUBJECT_ID  BIRTH_YEAR GENDER_FACTOR   RACE_FACTOR   ETHNICITY_FACTOR  \\\n",
            "0           1        2006   S1 - Female    R1 - Black  E0 - Non-Hispanic   \n",
            "1           2        1994   S1 - Female    R0 - White  E0 - Non-Hispanic   \n",
            "2           3        2006     S0 - Male    R0 - White      E1 - Hispanic   \n",
            "3           4        2004     S0 - Male  R4 - Unknown      E1 - Hispanic   \n",
            "4           5        2006   S1 - Female    R1 - Black  E0 - Non-Hispanic   \n",
            "\n",
            "        PAYER_FACTOR  ATOPIC_MARCH_COHORT  AGE_START_YEARS  AGE_END_YEARS  \\\n",
            "0      P1 - Medicaid                False         0.093087       3.164956   \n",
            "1  P0 - Non-Medicaid                False        12.232717      18.880219   \n",
            "2  P0 - Non-Medicaid                 True         0.010951       6.726899   \n",
            "3  P0 - Non-Medicaid                False         2.398357       9.111567   \n",
            "4  P0 - Non-Medicaid                False         0.013689       6.193018   \n",
            "\n",
            "   SHELLFISH_ALG_START  ...  CASHEW_ALG_END  ATOPIC_DERM_START  \\\n",
            "0                  NaN  ...             NaN                NaN   \n",
            "1                  NaN  ...             NaN                NaN   \n",
            "2                  NaN  ...             NaN           4.884326   \n",
            "3                  NaN  ...             NaN                NaN   \n",
            "4                  NaN  ...             NaN                NaN   \n",
            "\n",
            "   ATOPIC_DERM_END  ALLERGIC_RHINITIS_START  ALLERGIC_RHINITIS_END  \\\n",
            "0              NaN                      NaN                    NaN   \n",
            "1              NaN                      NaN                    NaN   \n",
            "2              NaN                 3.917864               6.157426   \n",
            "3              NaN                      NaN                    NaN   \n",
            "4              NaN                      NaN                    NaN   \n",
            "\n",
            "   ASTHMA_START  ASTHMA_END  FIRST_ASTHMARX  LAST_ASTHMARX  NUM_ASTHMARX  \n",
            "0           NaN         NaN             NaN            NaN           NaN  \n",
            "1           NaN         NaN       12.262834      18.880219           2.0  \n",
            "2      5.127995         NaN        1.404517       6.157426           4.0  \n",
            "3           NaN         NaN             NaN            NaN           NaN  \n",
            "4           NaN         NaN             NaN            NaN           NaN  \n",
            "\n",
            "[5 rows x 50 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Preprocessing\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Handle missing values (if any)\n",
        "df = df.dropna()  # Alternatively, you can use imputation techniques for missing values\n",
        "\n",
        "# Check if the column name \"Allergy\" exists before trying to map values\n",
        "# Print the DataFrame's columns to inspect their names\n",
        "print(df.columns)\n",
        "\n",
        "# Replace 'CorrectColumnName' with the actual name of the column representing allergy status\n",
        "# If the name is slightly different (case sensitivity, typo), correct it.\n",
        "# If the column needs renaming, rename it with df.rename(columns={'OriginalName': 'CorrectColumnName'}, inplace=True)\n",
        "correct_column_name = 'SHELLFISH_ALG_START'  # Replace with the actual column name\n",
        "\n",
        "if correct_column_name in df.columns:\n",
        "    df['Allergy'] = df['SHELLFISH_ALG_START'].map({'Yes': 1, 'No': 0})\n",
        "else:\n",
        "    print(f\"Column '{SHELLFISH_ALG_START}' not found in DataFrame. Please check your data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hwO4H45Ch7gd",
        "outputId": "14e04008-7401-4b50-8d4f-b092608e6576"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUBJECT_ID                 0\n",
            "BIRTH_YEAR                 0\n",
            "GENDER_FACTOR              0\n",
            "RACE_FACTOR                0\n",
            "ETHNICITY_FACTOR           0\n",
            "PAYER_FACTOR               0\n",
            "ATOPIC_MARCH_COHORT        0\n",
            "AGE_START_YEARS            0\n",
            "AGE_END_YEARS              0\n",
            "SHELLFISH_ALG_START        0\n",
            "SHELLFISH_ALG_END          0\n",
            "FISH_ALG_START             0\n",
            "FISH_ALG_END               0\n",
            "MILK_ALG_START             0\n",
            "MILK_ALG_END               0\n",
            "SOY_ALG_START              0\n",
            "SOY_ALG_END                0\n",
            "EGG_ALG_START              0\n",
            "EGG_ALG_END                0\n",
            "WHEAT_ALG_START            0\n",
            "WHEAT_ALG_END              0\n",
            "PEANUT_ALG_START           0\n",
            "PEANUT_ALG_END             0\n",
            "SESAME_ALG_START           0\n",
            "SESAME_ALG_END             0\n",
            "TREENUT_ALG_START          0\n",
            "TREENUT_ALG_END            0\n",
            "WALNUT_ALG_START           0\n",
            "WALNUT_ALG_END             0\n",
            "PECAN_ALG_START            0\n",
            "PECAN_ALG_END              0\n",
            "PISTACH_ALG_START          0\n",
            "PISTACH_ALG_END            0\n",
            "ALMOND_ALG_START           0\n",
            "ALMOND_ALG_END             0\n",
            "BRAZIL_ALG_START           0\n",
            "BRAZIL_ALG_END             0\n",
            "HAZELNUT_ALG_START         0\n",
            "HAZELNUT_ALG_END           0\n",
            "CASHEW_ALG_START           0\n",
            "CASHEW_ALG_END             0\n",
            "ATOPIC_DERM_START          0\n",
            "ATOPIC_DERM_END            0\n",
            "ALLERGIC_RHINITIS_START    0\n",
            "ALLERGIC_RHINITIS_END      0\n",
            "ASTHMA_START               0\n",
            "ASTHMA_END                 0\n",
            "FIRST_ASTHMARX             0\n",
            "LAST_ASTHMARX              0\n",
            "NUM_ASTHMARX               0\n",
            "Allergy                    0\n",
            "dtype: int64\n",
            "Index(['SUBJECT_ID', 'BIRTH_YEAR', 'GENDER_FACTOR', 'RACE_FACTOR',\n",
            "       'ETHNICITY_FACTOR', 'PAYER_FACTOR', 'ATOPIC_MARCH_COHORT',\n",
            "       'AGE_START_YEARS', 'AGE_END_YEARS', 'SHELLFISH_ALG_START',\n",
            "       'SHELLFISH_ALG_END', 'FISH_ALG_START', 'FISH_ALG_END', 'MILK_ALG_START',\n",
            "       'MILK_ALG_END', 'SOY_ALG_START', 'SOY_ALG_END', 'EGG_ALG_START',\n",
            "       'EGG_ALG_END', 'WHEAT_ALG_START', 'WHEAT_ALG_END', 'PEANUT_ALG_START',\n",
            "       'PEANUT_ALG_END', 'SESAME_ALG_START', 'SESAME_ALG_END',\n",
            "       'TREENUT_ALG_START', 'TREENUT_ALG_END', 'WALNUT_ALG_START',\n",
            "       'WALNUT_ALG_END', 'PECAN_ALG_START', 'PECAN_ALG_END',\n",
            "       'PISTACH_ALG_START', 'PISTACH_ALG_END', 'ALMOND_ALG_START',\n",
            "       'ALMOND_ALG_END', 'BRAZIL_ALG_START', 'BRAZIL_ALG_END',\n",
            "       'HAZELNUT_ALG_START', 'HAZELNUT_ALG_END', 'CASHEW_ALG_START',\n",
            "       'CASHEW_ALG_END', 'ATOPIC_DERM_START', 'ATOPIC_DERM_END',\n",
            "       'ALLERGIC_RHINITIS_START', 'ALLERGIC_RHINITIS_END', 'ASTHMA_START',\n",
            "       'ASTHMA_END', 'FIRST_ASTHMARX', 'LAST_ASTHMARX', 'NUM_ASTHMARX',\n",
            "       'Allergy'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split the dataset into features (X) and target (y)\n",
        "X = df.drop(['Allergy', 'AGE_END_YEARS'], axis=1)  # Assuming 'Allergy' is your target\n",
        "y = df['Allergy']\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BHuGj92iIMp",
        "outputId": "b9accfcb-ecc2-42cd-a07f-9ed4bb81e524"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (0, 49)\n",
            "Shape of y: (0,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Split the data into training and testing sets (80:20 ratio)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "f2_SZH-KiwXw",
        "outputId": "5d94a4e8-c60d-4d63-91f2-83731088b5b3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-49fc907fdba3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 4: Split the data into training and testing sets (80:20 ratio)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2784\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2785\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2786\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2415\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2416\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Standardize the features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "fqQP_o8Kizaf",
        "outputId": "29ad0523-bacb-4195-ec4f-70e886108fb3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-159bf6e43c09>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 5: Standardize the features (important for logistic regression)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2YStqwT3i1RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Classification Report (Precision, Recall, F1-score)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "XPokEtB5i4Rw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}