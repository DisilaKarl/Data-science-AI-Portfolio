{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, you will implement a fully functioning feedforward neural network in Python using only the NumPy library. You will not use any deep learning frameworks such as TensorFlow or PyTorch. Your goal is to train your neural network to learn the XOR function.\n",
        "Instructions\n",
        "1. Generate the training data for the XOR function using the following code snippet:\n",
        "----------------------------------------------------\n",
        "import numpy as np\n",
        "\n",
        "def generate_xor_data():\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    Y = np.array([[0], [1], [1], [0]])\n",
        "    return X, Y\n",
        "\n",
        "X, Y = generate_xor_data()\n",
        "-------------------------------------------------------\n",
        "2. Create a Python class called `NeuralNetwork` that will contain your neural network implementation. Your class should follow the template provided below:\n",
        "------------------------------------------------------\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize the weights and biases of the network\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Perform the forward pass and compute the output of the network\n",
        "        pass\n",
        "\n",
        "    def backward(self, X, Y, output):\n",
        "        # Perform the backward pass and compute the gradients of the loss function\n",
        "        pass\n",
        "\n",
        "    def update_weights(self, learning_rate):\n",
        "        # Update the weights and biases using the computed gradients\n",
        "        pass\n",
        "\n",
        "    def train(self, X, Y, epochs, learning_rate):\n",
        "        # Train the neural network using the provided training data\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Make predictions using the trained neural network\n",
        "        pass\n",
        "--------------------------------------------------------\n",
        "3. Implement the methods in the `NeuralNetwork` class following these guidelines:\n",
        "For weight initialization, use either Xavier or He initialization.\n",
        "Implement the forward pass, using the ReLU activation function for the hidden layer, and the sigmoid activation function for the output layer.\n",
        "Implement the backward pass using the chain rule and the gradients of the loss function with respect to the weights and biases.\n",
        "Use gradient descent or a variant thereof (e.g., mini-batch gradient descent, Adam, etc.) to update the weights and biases.\n",
        "Train your neural network on the XOR dataset, adjusting the number of epochs and learning rate as needed.\n",
        "4. Evaluate your trained neural network using the `predict` method and check if the predicted outputs match the actual XOR function outputs. You may also utilize some plots to help your visualize your results.\n",
        "\n",
        "Submission\n",
        "Submit a Python file containing your NeuralNetwork class implementation along with the code to generate the XOR data and train your neural network. Your submission should include comments to explain your implementation and any design choices you made. Additionally, include a brief report summarizing your results, including the network architecture, the number of epochs, the learning rate, and the final predictions."
      ],
      "metadata": {
        "id": "nvO6UkgLIP44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. XOR Data Generation"
      ],
      "metadata": {
        "id": "CTlDmDqbIZ_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EVueOA96IOOv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_xor_data():\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # XOR inputs\n",
        "    Y = np.array([[0], [1], [1], [0]])  # XOR outputs\n",
        "    return X, Y\n",
        "\n",
        "X, Y = generate_xor_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Neural Network Class Definition"
      ],
      "metadata": {
        "id": "BBgUXuFxIarl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize the weights and biases of the network using He initialization\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Weights and biases for input to hidden layer\n",
        "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2. / self.input_size)  # He Initialization\n",
        "        self.b1 = np.zeros((1, self.hidden_size))\n",
        "\n",
        "        # Weights and biases for hidden to output layer\n",
        "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2. / self.hidden_size)  # He Initialization\n",
        "        self.b2 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass to compute output using ReLU for hidden layer and Sigmoid for output layer\n",
        "        self.Z1 = np.dot(X, self.W1) + self.b1  # Weighted sum for hidden layer\n",
        "        self.A1 = np.maximum(0, self.Z1)  # ReLU activation for hidden layer\n",
        "\n",
        "        self.Z2 = np.dot(self.A1, self.W2) + self.b2  # Weighted sum for output layer\n",
        "        self.A2 = 1 / (1 + np.exp(-self.Z2))  # Sigmoid activation for output layer\n",
        "\n",
        "        return self.A2\n",
        "\n",
        "    def backward(self, X, Y, output):\n",
        "        # Backward pass to compute gradients using chain rule\n",
        "\n",
        "        # Compute the error in output\n",
        "        output_error = output - Y\n",
        "\n",
        "        # Compute gradient for weights and biases between hidden and output layer\n",
        "        dZ2 = output_error * output * (1 - output)  # Derivative of sigmoid\n",
        "        dW2 = np.dot(self.A1.T, dZ2)  # Gradient for W2\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)  # Gradient for b2\n",
        "\n",
        "        # Compute gradient for weights and biases between input and hidden layer\n",
        "        dA1 = np.dot(dZ2, self.W2.T)  # Backpropagated error to hidden layer\n",
        "        dZ1 = dA1 * (self.Z1 > 0)  # Derivative of ReLU\n",
        "        dW1 = np.dot(X.T, dZ1)  # Gradient for W1\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)  # Gradient for b1\n",
        "\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "    def update_weights(self, dW1, db1, dW2, db2, learning_rate):\n",
        "        # Update the weights and biases using gradient descent\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "\n",
        "    def train(self, X, Y, epochs, learning_rate):\n",
        "        # Train the neural network on the XOR dataset\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Backward pass\n",
        "            dW1, db1, dW2, db2 = self.backward(X, Y, output)\n",
        "\n",
        "            # Update weights\n",
        "            self.update_weights(dW1, db1, dW2, db2, learning_rate)\n",
        "\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(Y - output))  # Mean squared error loss\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Make predictions with the trained neural network\n",
        "        output = self.forward(X)\n",
        "        return (output > 0.5).astype(int)  # Return 1 if output > 0.5, else 0\n"
      ],
      "metadata": {
        "id": "BCYDU2JCIdWG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Training and Testing the Model"
      ],
      "metadata": {
        "id": "5H4XAbpXIgzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate XOR data\n",
        "X, Y = generate_xor_data()\n",
        "\n",
        "# Create the neural network instance\n",
        "input_size = 2  # Number of input features (for XOR: two inputs)\n",
        "hidden_size = 4  # Number of neurons in the hidden layer (can be adjusted)\n",
        "output_size = 1  # Output size (one output for XOR)\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Initialize the neural network\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(X, Y, epochs, learning_rate)\n",
        "\n",
        "# Test the network with the trained model\n",
        "predictions = nn.predict(X)\n",
        "\n",
        "# Print the results\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "print(\"Actual XOR Output:\")\n",
        "print(Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eUDP3HRIjvJ",
        "outputId": "a36bbd18-ad4f-47dc-9436-9376a887ef1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.34787185098008183\n",
            "Epoch 1000, Loss: 0.006868755653009804\n",
            "Epoch 2000, Loss: 0.0022212607090746433\n",
            "Epoch 3000, Loss: 0.0012232824413356677\n",
            "Epoch 4000, Loss: 0.0008275265037609213\n",
            "Epoch 5000, Loss: 0.0006195756618468821\n",
            "Epoch 6000, Loss: 0.0004923920127441533\n",
            "Epoch 7000, Loss: 0.00040713900528488103\n",
            "Epoch 8000, Loss: 0.0003463006931797388\n",
            "Epoch 9000, Loss: 0.0003007225173488562\n",
            "Predictions:\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n",
            "Actual XOR Output:\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ]
        }
      ]
    }
  ]
}