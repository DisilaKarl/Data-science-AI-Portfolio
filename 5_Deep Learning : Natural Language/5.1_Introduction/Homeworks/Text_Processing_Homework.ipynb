{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the homework assignment for text processing using NLTK. In this\n",
        "assignment, you will apply the text-processing techniques we have learned in\n",
        "the previous lessons using the Natural Language Toolkit (NLTK) library in\n",
        "Python.\n",
        "You will be given a set of text data and asked to perform various text\n",
        "processing tasks, including tokenization, stop word removal, stemming\n",
        "lemmatization, and regular expression matching. These tasks are everyday in\n",
        "natural language processing and are essential for preparing text data for\n",
        "analysis and modelling.\n",
        "By completing this assignment, you will gain hands-on experience in\n",
        "applying these techniques to real-world text data and become more\n",
        "proficient in using NLTK for text processing. Good luck!"
      ],
      "metadata": {
        "id": "j1AnefeZMAFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenize the following text into individual words:"
      ],
      "metadata": {
        "id": "NyF4_yTcMFpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq5_PgOGMYmF",
        "outputId": "170f68ef-d02e-47c0-a296-b8a421ccb9f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohdK8ouqLQo9",
        "outputId": "6ca79354-6318-4599-c5f6-791bfa83f40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Remove stop words from the following text:"
      ],
      "metadata": {
        "id": "aI70sWZqMwT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydeyYI6CMw7W",
        "outputId": "a716848e-fa68-47ca-ac46-ffba3dd34ff9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stem the following words using Porter Stemmer:"
      ],
      "metadata": {
        "id": "lfRjwoVsMy81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "words = [\"studies\", \"studying\", \"studied\", \"study\"]\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw374WQwM1vP",
        "outputId": "57a0b7b5-9399-4c5c-ebd2-6d15b5c0ffea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['studi', 'studi', 'studi', 'studi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Lemmatize the following words using WordNet Lemmatizer:"
      ],
      "metadata": {
        "id": "Bqal2supM44w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAM4z76IM5do",
        "outputId": "1caba278-fca0-446b-a5da-daa92f28e551"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['study', 'studying', 'studied', 'study']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Use regular expressions to extract all email addresses:"
      ],
      "metadata": {
        "id": "lciNpAWTM8fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Please contact us at info@example.com or support@example.com for further assistance.\"\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
        "emails = re.findall(email_pattern, text)\n",
        "print(emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bY5o7j0M81G",
        "outputId": "99bd2d62-9c09-498b-f0c2-c8caffb770ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['info@example.com', 'support@example.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Use regular expressions to extract all phone numbers:"
      ],
      "metadata": {
        "id": "NkZe-xXMNAXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phone_text = \"Please call us at (555) 123-4567 or (555) 987-6543 for further assistance.\"\n",
        "phone_pattern = r'\\(\\d{3}\\) \\d{3}-\\d{4}'\n",
        "phone_numbers = re.findall(phone_pattern, phone_text)\n",
        "print(phone_numbers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mTf7ltgNA-s",
        "outputId": "d7bd3e86-94d7-42e9-b1c7-cbf95b124d41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['(555) 123-4567', '(555) 987-6543']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Clean the following text by removing all punctuation and numbers, and converting all letters to lowercase:"
      ],
      "metadata": {
        "id": "riK5z29eNEbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text_to_clean = \"Hello, world! This is some text with some special characters like $ and %, and some extra whitespace. 123\"\n",
        "cleaned_text = ''.join([char.lower() for char in text_to_clean if char not in string.punctuation and not char.isdigit()])\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRT9pseaNFA2",
        "outputId": "8919435e-d339-474a-9f95-2942a1d0c93c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world this is some text with some special characters like  and  and some extra whitespace \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Convert the following text to a bag of words representation:"
      ],
      "metadata": {
        "id": "S_0R1YjONIXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bag_of_words_text = [\"The quick brown fox jumps over the lazy dog.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "bag_of_words = vectorizer.fit_transform(bag_of_words_text)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(bag_of_words.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH_7TR-iNKOg",
        "outputId": "5ec49b73-a3d2-4040-cd5d-2522fbb443bb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['brown' 'dog' 'fox' 'jumps' 'lazy' 'over' 'quick' 'the']\n",
            "[[1 1 1 1 1 1 1 2]]\n"
          ]
        }
      ]
    }
  ]
}