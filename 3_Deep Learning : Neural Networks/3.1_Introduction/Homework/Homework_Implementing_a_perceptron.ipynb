{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Homework: Implementing a Perceptron\n",
        "\n",
        "In this task, you will implement a simple perceptron model from scratch using Python. The goal of this task is to give you a hands-on experience of how a perceptron works and how it can be used for binary classification.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Define a function called `perceptron` that takes in three parameters:\n",
        "    - `X`: a numpy array of shape `(n_samples, n_features)` representing the input data.\n",
        "    - `y`: a numpy array of shape `(n_samples,)` representing the target labels (0 or 1).\n",
        "    - `eta`: the learning rate for updating the weights.\n",
        "\n",
        "2. Initialize the weight vector `w` to a random value of shape `(n_features,)`.\n",
        "\n",
        "3. Define a for loop that iterates over a specified number of epochs (e.g., 100). Within each epoch, iterate over all the training samples and do the following:\n",
        "    - Compute the predicted output `y_pred` by multiplying the input vector `X` with the weight vector `w` and passing it through a step function (e.g., Heaviside function).\n",
        "    - Compute the error `err` as the difference between the true labels `y` and predicted output `y_pred`.\n",
        "    - Update the weight vector `w` using the following formula: `w = w + eta * err * X[i]`\n",
        "\n",
        "4. After all epochs have been completed, return the learned weight vector `w`.\n",
        "\n",
        "5. Test your perceptron implementation on a simple binary classification problem, such as the XOR problem. Generate random data points and labels for the XOR problem and train your perceptron on this data. Print the learned weights and test the perceptron on new data points to see how well it can classify.\n",
        "\n",
        "Bonus:\n",
        "\n",
        "- Modify the perceptron to implement the perceptron learning algorithm with a bias term.\n",
        "- Modify the perceptron to implement the adaptive linear neuron (Adaline) algorithm.\n",
        "\n",
        "Deliverables:\n",
        "\n",
        "- A Jupyter notebook or Python script that implements the perceptron from scratch and solves the XOR problem.\n",
        "- A brief report explaining your implementation and results.\n",
        "\n",
        "Data:\n",
        "\n",
        "You can generate the XOR problem data using the following code:\n",
        "--------------------------------------------\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(100, 2)\n",
        "\n",
        "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
        "\n",
        "y = np.where(y, 1, -1)\n",
        "\n",
        "--------------------------------------------\n"
      ],
      "metadata": {
        "id": "G0oQFIv4C75N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function called perceptron that takes in three parameters\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def perceptron(X, y, eta=0.1, epochs=100):\n",
        "    # Initialise Weight vector\n",
        "    n_samples, n_features = X.shape\n",
        "    w = np.random.randn(n_features)\n",
        "    b = np.random.randn(1)\n",
        "\n",
        "   # Loop until all data points are correctly classified\n",
        "    while True:\n",
        "        # Variable to track if all points are classified correctly\n",
        "        errors = 0\n",
        "\n",
        "        # Loop through each training example\n",
        "        for i in range(len(y)):\n",
        "            # Calculate the prediction: weighted sum of inputs and weights\n",
        "            y_pred = np.sign(np.dot(X[i], w))  # Step function (Heaviside)\n",
        "\n",
        "            # Check if there's an error\n",
        "            if y_pred != y[i]:\n",
        "                # Update weights if the example is misclassified\n",
        "                w = w + eta * (y[i] - y_pred) * X[i]\n",
        "                errors += 1\n",
        "\n",
        "        # If no errors, it means everything is correctly classified\n",
        "        if errors == 0:\n",
        "            break\n",
        "\n",
        "    return w"
      ],
      "metadata": {
        "id": "JSQaEAcUDmFH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate XOR problem data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(100, 2)\n",
        "\n",
        "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
        "\n",
        "y = np.where(y, 1, -1)\n",
        "\n",
        "y = np.where(y, 1, -1)"
      ],
      "metadata": {
        "id": "joI3L8dYE-vz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the perceptron with a learning rate of 0.1\n",
        "w = perceptron(X, y, eta=0.1)"
      ],
      "metadata": {
        "id": "SAokrCzyFu27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the learned weights\n",
        "print(\"Learned weights:\", w)"
      ],
      "metadata": {
        "id": "VauWGLaJFxb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the perceptron on new data points\n",
        "test_points = np.array([[0.5, 0.5], [-0.5, -0.5], [0.5, -0.5], [-0.5, 0.5]])\n",
        "for point in test_points:\n",
        "    prediction = np.dot(point, w)\n",
        "    prediction = 1 if prediction >= 0 else -1\n",
        "    print(f\"Test point {point} -> Prediction: {prediction}\")"
      ],
      "metadata": {
        "id": "sMZHPMzGFtbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps followed:\n",
        "\n",
        "* Initialization of weights: I first initialized the model's weights with random values. These weights are used to weight the influence of each feature on the model's output.\n",
        "\n",
        "* Prediction: For each input data sample, I calculated a weighted sum of the features and then applied a threshold function (Heaviside function) to determine the predicted class (0 or 1) of the perceptron.\n",
        "\n",
        "* Error calculation: The error is the difference between the true value (label) and the perceptronâ€™s prediction. If the prediction is incorrect, the error is used to adjust the model's weights.\n",
        "\n",
        "* Weight update: I updated the weights based on the error, the learning rate, and the input data, following the perceptron update rule.\n",
        "\n",
        "* Testing: After training, I tested the model on new data to see how the perceptron made predictions"
      ],
      "metadata": {
        "id": "6xDpuLAEF3mA"
      }
    }
  ]
}